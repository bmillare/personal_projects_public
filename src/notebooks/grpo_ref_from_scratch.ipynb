{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d353b79-910b-4308-8254-190a50b01c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_log_softmax(logits, input_ids):\n",
    "    \"\"\"\n",
    "    Computes log probabilities for specific tokens in the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): The raw logits output from the model.\n",
    "        input_ids (torch.Tensor): The token IDs for which we want the log probabilities.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Log probabilities of the selected tokens.\n",
    "\n",
    "    Explanation:\n",
    "        1. Applies log softmax to convert logits to log probabilities over the vocabulary.\n",
    "        2. Uses gather to extract only the log probabilities corresponding to the input_ids.\n",
    "        3. Removes the extra dimension to match the original shape of input_ids.\n",
    "    \"\"\"\n",
    "    # BM: logits (bs, sl, vs)\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1) # BM: softmax across vocabulary\n",
    "    # BM: input_ids (bs, sl), unsqueeze(-1) -> (bs, sl, 1), which matches most dims of logits, gather happens at dim=-1 for both log_probs and ids\n",
    "    return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1) # BM: desired output: (bs, sl)\n",
    "\n",
    "# BM: note that this really is log of prob, not log of logits\n",
    "# - 1: we select for logits only for the tokens we actually generated\n",
    "# - 2: we use log_softmax to get actual log probability, and then only select this probability\n",
    "def compute_log_probs(model, input_ids, attention_mask, logits_to_keep):\n",
    "    \"\"\"\n",
    "    Computes the log probabilities for a batch of tokens.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        input_ids (torch.Tensor): Token IDs for input sequences.\n",
    "        attention_mask (torch.Tensor): Attention mask for input sequences.\n",
    "        logits_to_keep (int): Number of tokens to keep from the end of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Log probabilities of the selected tokens.\n",
    "\n",
    "    Explanation:\n",
    "        1. Gets logits from the model for the input sequence.\n",
    "        2. Selects logits for all tokens except the last one (as we predict next tokens).\n",
    "        3. Selects only the last 'logits_to_keep' tokens from both logits and input_ids.\n",
    "        4. Computes log probabilities for these tokens using selective_log_softmax.\n",
    "    \"\"\"\n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask).logits[:, :-1, :] # BM: ignore last token in seq, the generated token (not sure why)\n",
    "    input_ids = input_ids[:, -logits_to_keep:] # BM: logits_to_keep is based on completion_ids sl\n",
    "    logits = logits[:, -logits_to_keep:, :]\n",
    "    return selective_log_softmax(logits, input_ids) # BM: (bs, sl, vs), (bs, sl) -> (bs, sl)\n",
    "\n",
    "def create_completion_mask(completion_ids, eos_token_id):\n",
    "    \"\"\"\n",
    "    Creates a mask for completion tokens that excludes tokens after the EOS token.\n",
    "\n",
    "    Args:\n",
    "        completion_ids (torch.Tensor): Token IDs of the generated completions.\n",
    "        eos_token_id (int): The ID of the end-of-sequence token.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A binary mask with 1s for valid tokens and 0s after the EOS token.\n",
    "\n",
    "    Explanation:\n",
    "        1. Identifies positions where EOS tokens occur in each sequence.\n",
    "        2. Finds the index of the first EOS token in each sequence.\n",
    "        3. Creates a mask where positions before and including the first EOS are 1, others are 0.\n",
    "        4. If no EOS token is found in a sequence, all positions are set to 1.\n",
    "    \"\"\"\n",
    "    is_eos = completion_ids == eos_token_id\n",
    "    eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device)\n",
    "    mask_exists = is_eos.any(dim=1)\n",
    "    eos_idx[mask_exists] = is_eos.int().argmax(dim=1)[mask_exists]\n",
    "    sequence_indices = torch.arange(is_eos.size(1), device=completion_ids.device).expand(is_eos.size(0), -1)\n",
    "    return (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "def generate_completions(model, tokenizer, prompts, num_generations=4, max_completion_length=32):\n",
    "    \"\"\"\n",
    "    Generates multiple completions for each prompt.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for encoding and decoding text.\n",
    "        prompts (list): List of text prompts.\n",
    "        num_generations (int): Number of completions to generate per prompt.\n",
    "        max_completion_length (int): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Containing prompt IDs, prompt mask, completion IDs, and completion mask.\n",
    "\n",
    "    Explanation:\n",
    "        1. Encodes the prompts and moves them to the appropriate device.\n",
    "        2. Repeats each prompt num_generations times to generate multiple completions.\n",
    "        3. Generates completions using the model with specified parameters.\n",
    "        4. Extracts the completion IDs (excluding the prompt tokens).\n",
    "        5. Creates a mask for the completions using create_completion_mask.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "    prompt_ids = inputs[\"input_ids\"].to(device)\n",
    "    prompt_mask = inputs[\"attention_mask\"].to(device)\n",
    "    print(f\"Input batch size: {prompt_ids.size(0)}, Device before model: {prompt_ids.device}\")\n",
    "    prompt_length = prompt_ids.size(1)\n",
    "    prompt_ids = prompt_ids.repeat_interleave(num_generations, dim=0)\n",
    "    prompt_mask = prompt_mask.repeat_interleave(num_generations, dim=0)\n",
    "    # BM: AutoModelForCausalLM.from_pretrained\n",
    "    outputs = model.generate(\n",
    "        prompt_ids,\n",
    "        attention_mask=prompt_mask,\n",
    "        max_new_tokens=max_completion_length,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        early_stopping=False\n",
    "    )\n",
    "    print(f\"Output batch size: {outputs.size(0)}, Device after model: {outputs.device}\")\n",
    "    completion_ids = outputs[:, prompt_length:]\n",
    "    completion_mask = create_completion_mask(completion_ids, tokenizer.eos_token_id)\n",
    "    return prompt_ids, prompt_mask, completion_ids, completion_mask\n",
    "\n",
    "def generate_rollout_data(model, ref_model, tokenizer, batch_samples, num_generations, max_completion_length):\n",
    "    \"\"\"\n",
    "    Generates data for GRPO rollouts including completions and log probabilities.\n",
    "\n",
    "    Args:\n",
    "        model: The policy model being trained.\n",
    "        ref_model: The reference model for KL divergence calculation.\n",
    "        tokenizer: The tokenizer for encoding and decoding text.\n",
    "        batch_samples (list): Batch of training samples.\n",
    "        num_generations (int): Number of completions to generate per sample.\n",
    "        max_completion_length (int): Maximum completion length.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing all data needed for GRPO updates.\n",
    "\n",
    "    Explanation:\n",
    "        1. Extracts prompts and expected answers from the batch samples.\n",
    "        2. Generates completions using the current policy model.\n",
    "        3. Combines prompt and completion tokens.\n",
    "        4. Computes log probabilities from both the policy model and reference model.\n",
    "        5. Formats completions for reward calculation.\n",
    "        6. Repeats prompts and answers to match the number of generated completions.\n",
    "        7. Returns all data needed for GRPO loss calculation.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    prompts = [sample[\"prompt\"] if isinstance(sample, dict) else sample[0] for sample in batch_samples]\n",
    "    answers = [sample[\"answer\"] if isinstance(sample, dict) else sample[1] for sample in batch_samples]\n",
    "    with torch.no_grad():\n",
    "        prompt_ids, prompt_mask, completion_ids, completion_mask = generate_completions(\n",
    "            model, tokenizer, prompts, num_generations, max_completion_length\n",
    "        )\n",
    "        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "        logits_to_keep = completion_ids.size(1)\n",
    "        old_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)\n",
    "        ref_log_probs = compute_log_probs(ref_model, input_ids, attention_mask, logits_to_keep)\n",
    "    formatted_completions = [[{'content': tokenizer.decode(ids, skip_special_tokens=True)}] for ids in completion_ids]\n",
    "    repeated_prompts = [p for p in prompts for _ in range(num_generations)]\n",
    "    repeated_answers = [a for a in answers for _ in range(num_generations)]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"completion_mask\": completion_mask,\n",
    "        \"old_log_probs\": old_log_probs,\n",
    "        \"ref_log_probs\": ref_log_probs,\n",
    "        \"formatted_completions\": formatted_completions,\n",
    "        \"repeated_prompts\": repeated_prompts,\n",
    "        \"repeated_answers\": repeated_answers,\n",
    "        \"logits_to_keep\": logits_to_keep,\n",
    "        \"batch_size\": len(prompts),\n",
    "        \"num_generations\": num_generations\n",
    "    }\n",
    "\n",
    "def grpo_loss(model, ref_model, rollout_data, tokenizer, reward_function, beta=0.01, epsilon=0.2):\n",
    "    \"\"\"\n",
    "    Computes the GRPO loss for updating the policy model.\n",
    "\n",
    "    Args:\n",
    "        model: The policy model being trained.\n",
    "        ref_model: The reference model for KL divergence calculation.\n",
    "        rollout_data (dict): Data generated by generate_rollout_data.\n",
    "        tokenizer: The tokenizer for encoding and decoding text.\n",
    "        reward_function: Function that calculates rewards for completions.\n",
    "        beta (float): KL penalty coefficient.\n",
    "        epsilon (float): Clipping parameter for PPO.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The GRPO loss to be minimized.\n",
    "\n",
    "    Explanation:\n",
    "        1. Computes current token log probabilities using the policy model.\n",
    "        2. Calculates the probability ratio between current and old policies.\n",
    "        3. Computes rewards using the provided reward_function.\n",
    "        4. Calculates advantages by standardizing rewards within each prompt.\n",
    "        5. Computes the PPO surrogate objective with clipping.\n",
    "        6. Calculates the KL divergence between reference and policy models.\n",
    "        7. Combines surrogate loss and KL penalty.\n",
    "        8. Averages the loss across all tokens and batches.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = rollout_data[\"input_ids\"]\n",
    "    attention_mask = rollout_data[\"attention_mask\"]\n",
    "    completion_mask = rollout_data[\"completion_mask\"]\n",
    "    logits_to_keep = rollout_data[\"logits_to_keep\"]\n",
    "    old_log_probs = rollout_data[\"old_log_probs\"]\n",
    "    ref_log_probs = rollout_data[\"ref_log_probs\"]\n",
    "    # BM: maybe this is more (bs*ng, sl)\n",
    "    token_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep) #BM: I imagine we keep grad here, (bs*ng, sl)\n",
    "    ratio = torch.exp(token_log_probs - old_log_probs) # BM: (bs*ng, sl)\n",
    "    rewards = torch.tensor(\n",
    "        reward_function(prompts=rollout_data[\"repeated_prompts\"], completions=rollout_data[\"formatted_completions\"], answer=rollout_data[\"repeated_answers\"]),\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    )\n",
    "    #print(f\"Rewards: {rewards}\")  # Debug rewards\n",
    "    batch_size = rollout_data[\"batch_size\"]\n",
    "    num_generations = rollout_data[\"num_generations\"]\n",
    "    rewards = rewards.view(batch_size, num_generations) # BM: (batch_size * num_generations) -> (batch_size, num_generations)\n",
    "    avg_reward = rewards.mean().item() # BM: item just extracts single value out of tensor, this is just for printing\n",
    "    print(\"Average Reward:\", avg_reward)\n",
    "    mean_rewards = rewards.mean(dim=1).repeat_interleave(num_generations) # BM: (bs * ng), means for each batch are broadcasted/duplicated across ng\n",
    "    std_rewards = rewards.std(dim=1).repeat_interleave(num_generations) # BM: ditto\n",
    "    advantages = ((rewards.view(-1) - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1) # BM: remember, unsqueeze just wraps the desired dim in an extra dimension\n",
    "    # BM: (bs * ng) -> unsqueeze, a column matrix aka (bs*ng,1)\n",
    "    surr1 = ratio * advantages # BM: (bs*ng, sl) * (bs*ng,1) -> (bs*ng, sl)\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    surrogate_loss = torch.min(surr1, surr2)\n",
    "    kl = torch.exp(ref_log_probs - token_log_probs) - (ref_log_probs - token_log_probs) - 1\n",
    "    per_token_loss = surrogate_loss - beta * kl\n",
    "    loss = -((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n",
    "    return loss, avg_reward\n",
    "\n",
    "def train_with_grpo(model, tokenizer, train_data, num_iterations=1, num_steps=500, batch_size=4,\n",
    "                              num_generations=4, max_completion_length=128, beta=0.1,\n",
    "                              learning_rate=5e-6, mu=3, epsilon=0.2, reward_function=None, device_ids=None):\n",
    "    \"\"\"\n",
    "    This function is your original working code (train_with_grpo_static)\n",
    "    with an added outer loop for iterative GRPO updates per the pseudocode.\n",
    "\n",
    "    Args:\n",
    "        model: The language model to train.\n",
    "        tokenizer: The tokenizer for encoding and decoding text.\n",
    "        train_data (list): Training dataset.\n",
    "        num_iterations (int): Number of outer iterations (reference model updates).\n",
    "        num_steps (int): Number of batch updates per iteration.\n",
    "        batch_size (int): Number of prompts per batch.\n",
    "        num_generations (int): Number of completions per prompt.\n",
    "        max_completion_length (int): Maximum token length for completions.\n",
    "        beta (float): KL penalty coefficient.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        mu (int): Number of policy updates per batch.\n",
    "        epsilon (float): PPO clipping parameter.\n",
    "        reward_function: Function that calculates rewards for completions.\n",
    "        BM: ^ returns list of floats for each prompt-completion pair (I think batch_size * num_generations)\n",
    "        device_ids (list): List of GPU device IDs for DataParallel.\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "\n",
    "    Explanation:\n",
    "        1. For each outer iteration:\n",
    "           - Creates a reference model as a deep copy of the current policy model.\n",
    "           - Reinitializes the optimizer for the policy model.\n",
    "           - For each training step:\n",
    "             a. Samples a batch of examples from the training data.\n",
    "             b. Generates rollout data including completions and log probabilities.\n",
    "             c. For mu iterations:\n",
    "                i. Computes the GRPO loss.\n",
    "                ii. Updates the policy model using gradient descent.\n",
    "           - Monitors GPU memory usage and prints progress information.\n",
    "    \"\"\"\n",
    "    assert device_ids is not None and len(device_ids) > 1, \"This code needs at least 2 GPU cores to run!\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Wrap model with DataParallel if multiple GPUs are available.\n",
    "\n",
    "    model = nn.DataParallel(model, device_ids=device_ids)\n",
    "    print(f\"Model wrapped with DataParallel across GPUs: {device_ids}\")\n",
    "\n",
    "    # Outer loop: iterative GRPO updates.\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nIteration {iteration+1}/{num_iterations}\")\n",
    "\n",
    "        # Create a reference model (deep copy) and set it to eval mode.\n",
    "        ref_model = copy.deepcopy(model.module)\n",
    "        ref_model.eval()\n",
    "        for param in ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Reference model created.\")\n",
    "\n",
    "        # Reinitialize the optimizer for this iteration.\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        model.train()\n",
    "\n",
    "        # Inner loop: your original training steps.\n",
    "        for step in range(num_steps):\n",
    "            batch_samples = random.sample(train_data, batch_size)\n",
    "            with torch.no_grad():\n",
    "                rollout_data = generate_rollout_data(\n",
    "                    model.module,\n",
    "                    ref_model,\n",
    "                    tokenizer,\n",
    "                    batch_samples,\n",
    "                    num_generations,\n",
    "                    max_completion_length\n",
    "                )\n",
    "            for grpo_iter in range(mu):\n",
    "                loss, avg_reward = grpo_loss(\n",
    "                    model.module,\n",
    "                    ref_model,\n",
    "                    rollout_data,\n",
    "                    tokenizer,\n",
    "                    reward_function,\n",
    "                    beta=beta,\n",
    "                    epsilon=epsilon\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "                optimizer.step()\n",
    "                # Log to wandb\n",
    "                wandb.log({\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"average_reward\": avg_reward,\n",
    "                    \"iteration\": iteration + 1,\n",
    "                    \"step\": step + 1,\n",
    "                    \"grpo_iter\": grpo_iter + 1\n",
    "                })\n",
    "                print(f\"Iteration {iteration+1}/{num_iterations}, Step {step+1}/{num_steps}, \"\n",
    "                      f\"GRPO iter {grpo_iter+1}/{mu}, loss: {loss.item():.4f}\")\n",
    "                #for i in range(torch.cuda.device_count()):\n",
    "                #    print(f\"GPU {i} Usage: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MiB, \"\n",
    "                #          f\"Utilization: {torch.cuda.utilization(i)}%\")\n",
    "                # Uncomment to see the GPU utilization stats\n",
    "    return model.module"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
